** cpc contrastive predicive coding
*** nce loss 
note: 关于softmax：
- https://ruder.io/word-embeddings-softmax/

- https://www.tensorflow.org/extras/candidate_sampling.pdf

- __importance samplingm__

1 smaple logit

2 ”多标签“分类  -> {true_loss, smpale_loss ....}

3 sum over -1

#+begin_src bash
# sigmoid_cross_entropy_with_logits
 z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + log(1 + exp(-x))
= x - x * z + log(1 + exp(-x))
For x < 0, to avoid overflow in exp(-x):
 x - x * z + log(1 + exp(-x))
= log(exp(x)) - x * z + log(1 + exp(-x))
= - x * z + log(1 + exp(x))

Hence, to ensure stability and avoid overflow, the implementation uses this equivalent formulation
max(x, 0) - x * z + log(1 + exp(-abs(x)))

eg:
logits = tf.constant([1., -1., 0., 1., -1., 0., 0.])
labels = tf.constant([0., 0., 0., 1., 1., 1., 0.5])
tf.nn.sigmoid_cross_entropy_with_logits(
    labels=labels, logits=logits).numpy()
# detail：https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits


 #+end_src
 
 #+begin_src bash
 def _sum_rows(x):
  """Returns a vector summing up each row of the matrix x."""
  # _sum_rows(x) is equivalent to math_ops.reduce_sum(x, 1) when x is
  # a matrix.  The gradient of _sum_rows(x) is more efficient than
  # reduce_sum(x, 1)'s gradient in today's implementation. Therefore,
  # we use _sum_rows(x) in the nce_loss() computation since the loss
  # is mostly used for training.
  cols = array_ops.shape(x)[1]
  ones_shape = array_ops.stack([cols, 1])
  ones = array_ops.ones(ones_shape, x.dtype)
  return array_ops.reshape(math_ops.matmul(x, ones), [-1])
 #+end_src
 
**** [[https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/ops/nn_impl.py#L2007-L2109][tensorflow nce]]
** infonce loss
***  [[https://paperswithcode.com/method/infonce][infnce loss]]
***  https://crossminds.ai/video/the-infonce-loss-in-self-supervised-learning-606fef0bf43a7f2f827c1583/
*** [[https://zhuanlan.zhihu.com/p/334772391][math]]
** vq-vae
*** vq-wav2vec
*** kmeans 
*** gumbel sofmtax
** wav2vec2
*** deversity
*** G codebook
** hubert

