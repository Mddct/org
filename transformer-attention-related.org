** attention
- global relative

- local relative ([[https://arxiv.org/pdf/2005.04908.pdf][Local Self-Attention]])

- local

- global casual

- cross attention

local 和 local relative 区别是后者是加入了相对位置信息 [[https://zhuanlan.zhihu.com/p/344604604][wenet && transformer-xl && conformer 相对位置信息]]

#+begin_src python
    if atten_type == 'global_relative':
      atten_tpl = (
          attention_lib.MultiHeadedAttentionXL.Params().Set(
              rel_pos_emb_dim=relative_pos_emb_dim))
    elif atten_type == 'local_relative':
      atten_tpl = attention_lib.LocalSelfAttentionXL.Params().Set(
          left_context=atten_left_context,
          right_context=atten_right_context,
          rel_pos_emb_dim=relative_pos_emb_dim)
    elif atten_type == 'local':
      atten_tpl = attention_lib.LocalSelfAttention.Params().Set(
          left_context=atten_left_context, right_context=atten_right_context)
    else:
      atten_tpl = attention_lib.MultiHeadedAttention.Params()

#+end_src

#+begin_src python
## https://zhuanlan.zhihu.com/p/74485142
def rel_shift(x):
  x_size = tf.shape(x)

  x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])
  x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])
  x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])
  x = tf.reshape(x, x_size)

  return x

#+end_src

** pos 
- relative sinusoidal positional encoding scheme

** wenet mask tf
#+begin_src python
def tf_subsequent_chunk_mask(
        size,
        chunk_size,
        num_left_chunks=tf.constant(-1, dtype=tf.int32),
):

  index = tf.range(0,size)

  mask_seq = tf.minimum(((index // chunk_size) +1)*chunk_size , size)
  mask_1 = tf.sequence_mask(mask_seq, maxlen=size)

  def limit_left_fn():
    left_mask = tf.maximum((
        index // chunk_size - num_left_chunks)*chunk_size,0)
    
    mask_2 = tf.sequence_mask(left_mask, maxlen=size)
    return mask_1 & (~mask_2)

  return tf.cond(
      tf.not_equal(num_left_chunks, -1),
      limit_left_fn,
      lambda: mask_1,
    )
  #+end_src
