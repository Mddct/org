** attention
- global relative

- local relative ([[https://arxiv.org/pdf/2005.04908.pdf][Local Self-Attention]])

- local

- global casual

- cross attention

local 和 local relative 区别是后者是加入了相对位置信息 [[https://zhuanlan.zhihu.com/p/344604604][wenet && transformer-xl && conformer 相对位置信息]]

#+begin_src python
    if atten_type == 'global_relative':
      atten_tpl = (
          attention_lib.MultiHeadedAttentionXL.Params().Set(
              rel_pos_emb_dim=relative_pos_emb_dim))
    elif atten_type == 'local_relative':
      atten_tpl = attention_lib.LocalSelfAttentionXL.Params().Set(
          left_context=atten_left_context,
          right_context=atten_right_context,
          rel_pos_emb_dim=relative_pos_emb_dim)
    elif atten_type == 'local':
      atten_tpl = attention_lib.LocalSelfAttention.Params().Set(
          left_context=atten_left_context, right_context=atten_right_context)
    else:
      atten_tpl = attention_lib.MultiHeadedAttention.Params()

#+end_src

#+begin_src python
## https://zhuanlan.zhihu.com/p/74485142
def rel_shift(x):
  x_size = tf.shape(x)

  x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])
  x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])
  x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])
  x = tf.reshape(x, x_size)

  return x

#+end_src

** pos 
- relative sinusoidal positional encoding scheme

** wenet mask tf
#+begin_src python
def tf_subsequent_chunk_mask(
        size,
        chunk_size,
        num_left_chunks=tf.constant(-1, dtype=tf.int32),
):

  index = tf.range(0,size)

  mask_seq = tf.minimum(((index // chunk_size) +1)*chunk_size , size)
  mask_1 = tf.sequence_mask(mask_seq, maxlen=size)

  def limit_left_fn():
    left_mask = tf.maximum((
        index // chunk_size - num_left_chunks)*chunk_size,0)
    
    mask_2 = tf.sequence_mask(left_mask, maxlen=size)
    return mask_1 & (~mask_2)

  return tf.cond(
      tf.not_equal(num_left_chunks, -1),
      limit_left_fn,
      lambda: mask_1,
    )
  #+end_src

#+begin_src python
@tf.function
def add_optional_chunk_mask(xs: tf.Tensor, masks: tf.Tensor,
                            use_dynamic_chunk: bool,
                            use_dynamic_left_chunk: bool,
                            decoding_chunk_size: int, static_chunk_size: int,
                            num_decoding_left_chunks: int):
  
  
  if use_dynamic_chunk:
        max_len = xs.size(1)
        if decoding_chunk_size < 0:
            chunk_size = max_len
            num_left_chunks = -1
        elif decoding_chunk_size > 0:
            chunk_size = decoding_chunk_size
            num_left_chunks = num_decoding_left_chunks
        else:
            chunk_size = tf.random.uniform(
                shape=[],
                 minval=1, 
                 maxval=max_len, 
                 dtype=tf.int32)
            num_left_chunks = -1
            if chunk_size > max_len // 2:
                chunk_size = max_len
            else:
                chunk_size = chunk_size % 25 + 1
                if use_dynamic_left_chunk:
                    max_left_chunks = (max_len - 1) // chunk_size
                    num_left_chunks = tf.random.uniform(
                        shape=[], 
                        minval=0, 
                        maxval=max_left_chunks, 
                        dtype=tf.int32)   

  elif static_chunk_size > 0:
      num_left_chunks = num_decoding_left_chunks
  else:
      return masks

  chunk_size =  tf.convert_to_tensor(chunk_size) 
  num_left_chunks = tf.convert_to_tensor(num_left_chunks)

  chunk_masks = tf_subsequent_chunk_mask(xs.shape[1], chunk_size,
                                        num_left_chunks)  # (L, L)
  chunk_masks =  tf.expand_dims(chunk_masks, 0) #[1,L, L]
  chunk_masks = masks & chunk_masks  # (B, L, L)

  return chunk_masks
#+end_src
