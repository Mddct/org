** attention
- global relative

- local relative ([[https://arxiv.org/pdf/2005.04908.pdf][Local Self-Attention]])

- local

- global casual

- cross attention

local 和 local relative 区别是后者是加入了相对位置信息 [[https://zhuanlan.zhihu.com/p/344604604][wenet && transformer-xl && conformer 相对位置信息]]

#+begin_src python
    if atten_type == 'global_relative':
      atten_tpl = (
          attention_lib.MultiHeadedAttentionXL.Params().Set(
              rel_pos_emb_dim=relative_pos_emb_dim))
    elif atten_type == 'local_relative':
      atten_tpl = attention_lib.LocalSelfAttentionXL.Params().Set(
          left_context=atten_left_context,
          right_context=atten_right_context,
          rel_pos_emb_dim=relative_pos_emb_dim)
    elif atten_type == 'local':
      atten_tpl = attention_lib.LocalSelfAttention.Params().Set(
          left_context=atten_left_context, right_context=atten_right_context)
    else:
      atten_tpl = attention_lib.MultiHeadedAttention.Params()

#+end_src

#+begin_src python
## https://zhuanlan.zhihu.com/p/74485142
def rel_shift(x):
  x_size = tf.shape(x)

  x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])
  x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])
  x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])
  x = tf.reshape(x, x_size)

  return x

#+end_src

** pos 
- relative sinusoidal positional encoding scheme
